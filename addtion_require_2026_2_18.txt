[0:09 - 0:14] Bren & Client:  Hello. Hey, Bren. How are you guys doing?
[0:15 - 0:16] Client: I'm good, buddy. How are you?
[0:17 - 0:25] Bren: Not too bad. You know, all things considered, I'm still above ground. It's cold as heck up here, but
[0:26 - 0:27] Bren: how's the weather?
[0:27 - 0:28] Client: What's the weather like? What is the temperature?
[0:28 - 0:32] Bren: It's like minus 25, I think they said.
[0:33 - 0:37] Client: Oh, wow. You're in Alaska, aren't you? Correct.
[0:37 - 0:38] Bren: Correct.
[0:39 - 0:48] Client: Okay. All right. No, we're in California. We're complaining. It's 55 and we're complaining. It's a summer's day out there.
[0:50 - 1:20] Bren: Yeah, exactly. So I wanted to just kind of set what I've been up to. I've been focused on building the GPU virtualization, obviously from the back and forth, but And I don't know how deep everybody that's on the call has been up to speed. So obviously that's what I'm working on. Also the mediation layer, and I've been doing that for the past nine weeks.
[1:21 - 1:58] Bren:  And kind of proved out the architecture, this multi-VM GPU sharing on the H100 with the priority-based scheduling, the pool isolation, the PCI-MMIO-based transport. which replaced the old NFS approach. Foundation seems solid, and now at the point where, you know, iteration speed is going to accelerate as we move into the, you know, the next parts with the advanced scheduling, class stack integration, and hardening phases. Right.
[1:58 - 2:11] Client:  Right. So let me give you a bit of background as well, Bren, just so you're aware. So, we develop a bunch of different technologies. One of them being post-quantum proofing. That's something that I have another team working on.
[2:12 - 2:36] Client:  We develop our own GPU processes as well. And what I wanted, part of our business is we build data centers and the government uses our data centers. Our biggest customer is the government. Different divisions of the government. Can't really go too much into that. But what we found is is our GPUs for most of the time are sitting idle. And we had this idea, this crazy idea.
[2:37 - 3:09] Client:  Why can't we virtualize the GPU? Because NVIDIA don't support that in its native format anyway. So what the plan was is when we virtualize one of our machines, and we're a partner with Intel, so we're developing our own Blade server. What we want to do is we want to have a single or a dual GPU inside a machine. and then we allow multiple users on that machine. We could have 100 virtualization clients on that machine.
[3:09 - 3:32] Client:  And we have this layer which is invisible to those customers. So everybody thinks they have their own GPU. And the layer simply redirects those function requests to the hardware and then returns the data back to that particular instance that made that call.
[3:33 - 3:46] Client:  So the three of us might each be on that single machine. I have the highest priority. I have made it make a request to the GPU.
[3:47 - 4:06] Client:  It goes into our layer. Then it's my turn. So that function is then processed on the GPU and the data is then returned to me when it's finished and then we start the cycle again. So that's essentially what we're looking to do.
[4:07 - 4:07] Bren:  Got it.
[4:10 - 4:33] Bren: And excuse me. So have you had any initial thoughts? Because that lines up really closely with what you know, what I've been doing, there's this mediation layer that makes the GPU appear dedicated per VM while scheduling access under the hood. Are you mainly targeting that for your HyperVault or the data center deployments?
[4:34 - 4:52] Client:  It could be either. DataVaults are essentially a mobile data center, so it could be one of those. But it could be either a data center, or a mobile data center, we call it Data Vault. 
[4:53 - 5:00] Bren: Got it. And so what are your outstanding concerns? Because all that makes sense.
[5:01 - 5:25] Client: Right. I want it to be clear that's what we want, because I don't want you to go, oh, I wish you had told me this. So I know for... for sure you're on the right lines. What I want to kind of establish with you is that this layer that you're developing, it's invisible to the instance user. That's what I wanted to ensure.
[5:26 - 5:39] Momik: And one more thing to this, Bren. We want our underlying hardware visible and our virtual GPU, whatever we can call it, it's visible to the user, no issue.
[5:39 - 5:54] Bren:  Yeah. Yeah, so that's, yeah, exactly. That's already how it works. So each instance sees what looks like a dedicated GPU, but all the scheduling and routing happen transparently in the mediation layer.
[5:54 - 6:25] Momik:  Yeah, and one more thing, actually we have a cloud layer, but for development and speed up the process and testing, we can do, we can just implement through our hypervisor and the VM. Through backend, we have the provisioning to attach it to the cloud VM, but that functionality is essential, but it can be developed later. But right now, our focus should be how our VM interacts with the mediation layer.
[6:25 - 6:52] Momik:  For example, generally, user will run Olama model, transformers, how that interacts, how that calls are passed and sent to the mediation layer and how the results are sent back. So next step is this task so we can have a model running in our Lama and the actual GPU take that request and perform the computation on the physical GPU and give that result to the VM.
[6:54 - 6:54] Bren:  Got it.
[6:57 - 7:13] Bren: do you have any questions for us? Yeah, I've got a few, actually. I mean, it sounds like we're... We're on the same page. But as far as the initial Olama integration, do you want me to target a specific model or just validate the GPU call path end-to-end first?
[7:15 - 7:44] Momik:  If you integrate Olama, there are hundreds of thousands of models that can be run and almost 90-95% are covered. So target only Olama, but to be on specific, you can use Lama 3.2, you can use Olama, Clawed, you can use Quen, whatever you feel like it. But Olama is much better for testing.
[7:45 - 7:46] Bren:  Sure, of course.
[7:49 - 8:16] Bren: And do you want the Olama integration purely for the internal testing or part of the production cloud stack path? That's one question. And then the tenant isolation, the workloads, well, I guess if these workloads are trusted internal users or they get to be external tenants, that's going to determine when we prioritize the IOM hardening, right?
[8:16 - 8:17] Bren:  If that makes sense. Yeah.
[8:18 - 8:48] Momik: Yeah. Actually, the first and foremost thing is to achieve the functionality. just that Olama integration with the physical GPU through that mediation layer. That's the first and foremost thing. But our use case will be both for cloud, for internal user, but mainly for the tenants and the cloud users. So we need that type of hardened security. But we can add that security. We can add that layer in our cloud.
[8:50 - 9:00] Momik:  First thing is to develop that functionality. that our VM with Olama running a model and it's purely computing through virtual GPU.
[9:02 - 9:13] Momik:  Yep. And scheduling, we can also take care of it. Once this is done, next step is scheduling and hardening. And then we go for the cloud. Yeah,
[9:14 - 9:31] Bren: yeah. That's basically what I was going to say. So I'll focus first on getting that NN Olama model, execute working through that mediation layer, And then layer in the cloud integration and IOMMU, the hardening once that path is, we have that validated. Make sense?
[9:31 - 9:49] Momik:  Yeah. Yeah. And don't scheduling and hardening. These are important, but it takes time to mature. And we don't want to waste so much time on that first because we need to achieve functionality first. Then later we will harden it. and we will schedule it as well.
[9:49 - 9:56] Momik:  And West Tudor will also take care of scheduling to some extent, so we can use that as well.
[9:58 - 10:22] Bren: Yeah, got it. I completely understand. So functionality first, and then we'll iterate on the scheduling and hardening once this core path is validated, and we'll still keep our momentum while we get aligned on the long-term plan. Yeah. Everything sound like we're on the same page because it sounds like we are.
[10:23 - 10:24] Client:  Thank you, guys.
[10:25 - 10:25] Bren: Is
[10:25 - 10:27] Client: that it? That's it for me.
[10:29 - 10:53] Momik: Anything else, Ben? If you need any clarity or anything, just drop a message. We'll be responding to you. And VMs are available. Hardware are there. Just focus on that. Achieve that functionality, and we will build on that. So we will be safe. Mentally, we are safe that we have done it. Now we will improve it. Sure, sure.
[10:53 - 11:07] Bren:  All right, I will send you â€“ I know I have some questions. They're just not jumping out in my head, of course. As soon as we get off the phone, I'll remember them. But I'll send them to you. Thanks, man. Okay. All right, thanks, guys. See you.