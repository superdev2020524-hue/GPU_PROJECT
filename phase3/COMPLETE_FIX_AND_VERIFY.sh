#!/bin/bash
# Complete fix and verification script for test-4 VM
# Run this on the VM: bash COMPLETE_FIX_AND_VERIFY.sh

set -e

echo "=========================================="
echo "COMPLETE FIX AND VERIFICATION"
echo "=========================================="
date
echo ""

# Step 1: Clean
echo "[1/8] Cleaning up..."
sudo systemctl stop ollama 2>&1 || true
sudo pkill -9 ollama 2>&1 || true
sudo rm -f /etc/ld.so.preload
sleep 2
echo "✓ Cleaned"

# Step 2: Install Ollama if needed
echo ""
echo "[2/8] Checking Ollama installation..."
if [ ! -f /usr/local/bin/ollama ]; then
    echo "  Installing Ollama..."
    curl -fsSL https://ollama.com/install.sh | sh
    sleep 5
fi
/usr/local/bin/ollama --version 2>&1 || echo "  Version check failed"
echo "✓ Ollama checked"

# Step 3: Make executable
echo ""
echo "[3/8] Ensuring binary is executable..."
sudo chmod +x /usr/local/bin/ollama
echo "✓ Binary executable"

# Step 4: Fix service
echo ""
echo "[4/8] Fixing service configuration..."
sudo mkdir -p /etc/systemd/system/ollama.service.d
cat << EOF | sudo tee /etc/systemd/system/ollama.service.d/override.conf
[Service]
Type=simple
Restart=always
RestartSec=5
TimeoutStartSec=300
EOF
sudo mkdir -p /usr/share/ollama /var/lib/ollama
sudo chmod 755 /var/lib/ollama
sudo systemctl daemon-reload
echo "✓ Service configured"

# Step 5: Start Ollama
echo ""
echo "[5/8] Starting Ollama..."
sudo systemctl start ollama
sleep 20

if systemctl is-active --quiet ollama; then
    echo "✓ Ollama is running"
    OLLAMA_PID=$(pgrep -f "ollama serve" | head -1)
    echo "  PID: $OLLAMA_PID"
else
    echo "✗ Ollama failed to start"
    echo ""
    echo "Error details:"
    sudo journalctl -u ollama -n 100 --no-pager | tail -50
    exit 1
fi

# Step 6: Configure preload
echo ""
echo "[6/8] Configuring preload..."
echo "/usr/lib64/libvgpu-cuda.so" | sudo tee /etc/ld.so.preload > /dev/null
echo "✓ Preload configured:"
cat /etc/ld.so.preload

# Step 7: Restart with preload
echo ""
echo "[7/8] Restarting with preload..."
sudo systemctl restart ollama
sleep 15

if systemctl is-active --quiet ollama; then
    echo "✓ Ollama running with shim"
    
    # Check shim loading
    OLLAMA_PID=$(pgrep -f "ollama serve" | head -1)
    if sudo cat /proc/$OLLAMA_PID/maps 2>&1 | grep -q "libvgpu-cuda"; then
        echo "  ✓ Shim library loaded in process"
    else
        echo "  ⚠ Shim library not found in process maps"
    fi
    
    # Check shim log
    if [ -f "/tmp/vgpu-shim-cuda-${OLLAMA_PID}.log" ]; then
        echo "  ✓ Shim log file exists"
        if grep -q "Pre-initialization succeeded" "/tmp/vgpu-shim-cuda-${OLLAMA_PID}.log"; then
            echo "  ✓ Pre-initialization successful"
        fi
    fi
else
    echo "✗ Ollama failed with preload"
    sudo journalctl -u ollama -n 50 --no-pager | tail -20
    exit 1
fi

# Step 8: Test and verify
echo ""
echo "[8/8] Testing and verifying GPU mode..."
echo "  Running test inference..."
timeout 45 ollama run llama3.2:1b "hello" 2>&1 | head -10

echo ""
echo "  Checking library mode..."
LIBRARY_MODE=$(sudo journalctl -u ollama --since "4 minutes ago" --no-pager 2>&1 | grep -E "library=" | tail -5)

echo ""
echo "=========================================="
echo "FINAL RESULTS"
echo "=========================================="
echo ""
echo "Library mode entries:"
echo "$LIBRARY_MODE"
echo ""

if echo "$LIBRARY_MODE" | grep -qi "library=cuda"; then
    echo "=========================================="
    echo "✓✓✓ SUCCESS! OLLAMA IS USING GPU MODE! ✓✓✓"
    echo "=========================================="
    echo ""
    echo "Summary:"
    echo "  ✓ Ollama service running"
    echo "  ✓ Shim library loaded"
    echo "  ✓ Thread-safe cuInit fix active"
    echo "  ✓ Ollama recognizing vGPU"
    echo "  ✓ Operating in GPU mode (library=cuda)"
    echo ""
    echo "The vGPU is now fully operational with Ollama!"
    exit 0
elif echo "$LIBRARY_MODE" | grep -qi "library=cpu"; then
    echo "⚠ Still using CPU mode"
    echo "May need to run another inference or check shim loading"
    exit 1
else
    echo "? Could not determine library mode"
    echo "Run another inference: ollama run llama3.2:1b 'test'"
    echo "Then check: sudo journalctl -u ollama --since '1 minute ago' | grep library="
    exit 2
fi
